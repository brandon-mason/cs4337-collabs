{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yMIsc30yf_0w",
        "SmpTHqSoi3Wx",
        "U_CINer1mHT2",
        "wHWlg3AWgUHx"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brandon-mason/cs4337-collabs/blob/main/QuestionWarmUpPython.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment Introduction:\n",
        "\n",
        "In this assignment, the objective is to assess and enhance your Python programming skills by building a comprehensive data analysis project. You will develop a `DataConfig` class for generating synthetic data and an `AdvancedDataAnalyzer` class for performing various operations like statistical analysis, data filtering, and visualization. The project is designed to test your understanding of object-oriented programming (OOP), decorators, and the use of key libraries such as `pandas`, `numpy`, `matplotlib`, and `param`.\n",
        "\n",
        "You are expected to approach the project incrementally. Begin by developing small parts of the functionality, testing each as you go. Utilize Google Colab's scratch-code cells feature for this purpose, where you can write temporary test code for each function before moving on to the next. Incremental development and testing will help ensure that each component works correctly before combining them into the final project."
      ],
      "metadata": {
        "id": "Ass0KapCwRl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RUBRICS**\n",
        "\n",
        "### **Code Completion**\n",
        "\n",
        "| **Code Completion**            |                                                                                                       |\n",
        "|--------------------------------|-------------------------------------------------------------------------------------------------------|\n",
        "| **Excellent (90-100%)**        | All functions fully implemented as required, including decorators, data generation, analysis, filtering, and saving functionalities. |\n",
        "| **Good (70-89%)**              | Most functions implemented with minor missing details or errors.                                       |\n",
        "| **Satisfactory (50-69%)**      | Partial implementation, significant parts missing or incomplete, but the core structure is present.    |\n",
        "| **Needs Improvement (0-49%)**  | Minimal implementation, many core functions missing or non-functional.                                |\n",
        "| **Weight**                     | 50%                                                                                                  |\n",
        "\n",
        "---\n",
        "\n",
        "### **Unit Test Pass**\n",
        "\n",
        "| **Unit Test Pass**             |                                                                                                       |\n",
        "|--------------------------------|-------------------------------------------------------------------------------------------------------|\n",
        "| **Excellent (90-100%)**        | All unit tests pass successfully with no errors.                                                      |\n",
        "| **Good (70-89%)**              | Most unit tests pass, minor issues with a few test cases.                                              |\n",
        "| **Satisfactory (50-69%)**      | Some unit tests pass, but significant errors or test failures occur.                                   |\n",
        "| **Needs Improvement (0-49%)**  | Most or all unit tests fail, indicating major issues with the implementation.                          |\n",
        "| **Weight**                     | 30%                                                                                                  |\n",
        "\n",
        "---\n",
        "\n",
        "### **Docstrings and Comments**\n",
        "\n",
        "| **Docstrings and Comments**     |                                                                                                       |\n",
        "|---------------------------------|-------------------------------------------------------------------------------------------------------|\n",
        "| **Excellent (90-100%)**         | All functions are properly documented with clear, concise docstrings explaining the purpose, inputs, and outputs. In-code comments are provided where necessary. |\n",
        "| **Good (70-89%)**               | Most functions have proper docstrings, but a few are missing or unclear. Comments are mostly appropriate. |\n",
        "| **Satisfactory (50-69%)**       | Docstrings are present but incomplete or unclear. Minimal in-code comments.                            |\n",
        "| **Needs Improvement (0-49%)**   | Little to no docstrings or comments provided, or comments are incorrect/misleading.                    |\n",
        "| **Weight**                      | 20%                                                                                                   |\n",
        "\n",
        "These three separate tables provide a clear breakdown of the assignment grading criteria for code completion, unit test success, and documentation."
      ],
      "metadata": {
        "id": "Dk98nHiXw5qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Structure"
      ],
      "metadata": {
        "id": "yMIsc30yf_0w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hereâ€™s a detailed guide explaining how to tackle each function and class in your implementation, along with usage examples, expected inputs, and outputs.\n",
        "\n",
        "### 1. **Class: `DataConfig`**\n",
        "\n",
        "The `DataConfig` class is responsible for configuring and generating synthetic data based on various parameters. It automatically regenerates data whenever any configuration parameters change.\n",
        "\n",
        "#### Key Parameters:\n",
        "- `random_seed`: Sets the random seed for reproducibility.\n",
        "- `num_samples`: Number of samples to generate (default 1000).\n",
        "- `age`: Range of ages (tuple of floats, default `(18.0, 80.0)`).\n",
        "- `height`: Height values (tuple of floats, default `(165.0, 10.0)` for mean and standard deviation).\n",
        "- `weight`: Weight values (tuple of floats, default `(70.0, 15.0)`).\n",
        "- `income`: Income values (tuple of floats, default `(50000.0, 15000.0)`).\n",
        "- `expenditure`: Expenditure values as a factor of income (tuple of floats, default `(0.6, 0.1)`).\n",
        "- `gender`: List selector for gender options, including `'Male'`, `'Female'`, and `'Other'`.\n",
        "\n",
        "#### Functions:\n",
        "- **_generate()**: Automatically generates synthetic data whenever a parameter changes.\n",
        "- **data**: Property that returns the generated dataset (a pandas DataFrame).\n",
        "\n",
        "#### Usage:\n",
        "\n",
        "```python\n",
        "# Initialize DataConfig with default parameters\n",
        "config = DataConfig()\n",
        "\n",
        "# Access the generated data\n",
        "data = config.data\n",
        "print(data.head())\n",
        "\n",
        "# Change the number of samples and regenerate the data\n",
        "config.num_samples = 500\n",
        "new_data = config.data\n",
        "print(len(new_data))  # Should print 500 as the new length\n",
        "```\n",
        "\n",
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "    Age      Height      Weight        Income  Expenditure  Gender\n",
        "0   50  168.248691   80.000451  55641.779379  31260.843872    Male\n",
        "1   69  171.232334   88.092898  48825.385468  28636.106034  Female\n",
        "2   60  169.732534   53.858662  60739.667523  35178.519206    Male\n",
        "3   28  163.592334   61.824884  44189.589121  26071.507426  Female\n",
        "4   73  151.606634   75.938644  33085.981912  18912.423281  Female\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Class: `AdvancedDataAnalyzer`**\n",
        "\n",
        "This class is built around the `DataConfig` object and provides analysis, filtering, visualization, and data saving functionality.\n",
        "\n",
        "#### Key Methods:\n",
        "- **calculate_statistics()**: Calculates and returns statistics like mean, median, variance, and correlation matrix.\n",
        "- **group_by_column(column)**: Groups the data by the specified column and returns aggregated means.\n",
        "- **apply_function(column, func)**: Applies a custom function to the specified column (e.g., lambda function).\n",
        "- **filter_data(column, condition)**: Filters data based on a condition applied to a column.\n",
        "- **lazy_filter(column, condition)**: Lazily filters data (yields results one at a time) based on a condition.\n",
        "- **visualize_relationship(column_x, column_y)**: Plots a scatter plot to visualize the relationship between two columns.\n",
        "- **visualize_distribution(column)**: Plots a histogram of the specified column to visualize the distribution.\n",
        "- **save_data(format, file_path, **kwargs)**: Saves the data to disk in the specified format (`csv`, `parquet`, or `npz`).\n",
        "\n",
        "#### Usage:\n",
        "\n",
        "```python\n",
        "# Initialize DataConfig and AdvancedDataAnalyzer\n",
        "config = DataConfig()\n",
        "analyzer = AdvancedDataAnalyzer(config)\n",
        "\n",
        "# Calculate statistics\n",
        "stats = analyzer.calculate_statistics()\n",
        "print(stats)\n",
        "\n",
        "# Group by the 'Gender' column and return means\n",
        "grouped = analyzer.group_by_column('Gender')\n",
        "print(grouped)\n",
        "\n",
        "# Apply a 10% increase to the 'Income' column\n",
        "increased_income = analyzer.apply_function('Income', lambda x: x * 1.1)\n",
        "print(increased_income.head())\n",
        "\n",
        "# Filter data where 'Age' is greater than 50\n",
        "filtered_data = analyzer.filter_data('Age', lambda x: x > 50)\n",
        "print(filtered_data.head())\n",
        "\n",
        "# Save the data as a CSV file\n",
        "analyzer.save_data(format='csv', file_path=\"synthetic_data\")\n",
        "```\n",
        "\n",
        "#### Example Output:\n",
        "\n",
        "1. **Statistics Calculation:**\n",
        "    ```python\n",
        "    {\n",
        "        'mean': {'Age': 49.5, 'Height': 165.42, 'Weight': 70.55, 'Income': 50000.23},\n",
        "        'median': {'Age': 50, 'Height': 165.01, 'Weight': 70.34, 'Income': 49999.89},\n",
        "        'variance': {'Age': 304.55, 'Height': 12.34, 'Weight': 15.67, 'Income': 22500123.45},\n",
        "        'correlation_matrix': {'Age': {'Age': 1.0, 'Income': 0.02}, 'Height': {'Height': 1.0}}\n",
        "    }\n",
        "    ```\n",
        "\n",
        "2. **Grouping by Column (`Gender`):**\n",
        "    ```python\n",
        "                Age      Height     Weight        Income\n",
        "    Gender                                          \n",
        "    Female  49.213    164.985      71.048   51000.123\n",
        "    Male    50.456    165.876      69.893   49000.321\n",
        "    Other   48.765    165.332      70.123   49500.456\n",
        "    ```\n",
        "\n",
        "3. **Filtered Data (Age > 50):**\n",
        "    ```python\n",
        "        Age      Height      Weight        Income  Expenditure  Gender\n",
        "    2   51  163.43          72.32        50234.2    30140.3       Male\n",
        "    4   55  162.51          74.29        51923.1    31015.9       Other\n",
        "    ```\n",
        "\n",
        "4. **File Saved Output (save as CSV):**\n",
        "    ```python\n",
        "    Data saved to synthetic_data.csv\n",
        "    ```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Data Visualization Functions**\n",
        "These functions help visualize relationships and distributions in your dataset.\n",
        "\n",
        "#### Usage:\n",
        "\n",
        "```python\n",
        "# Visualize relationship between 'Income' and 'Expenditure'\n",
        "analyzer.visualize_relationship('Income', 'Expenditure')\n",
        "\n",
        "# Visualize the distribution of 'Age'\n",
        "analyzer.visualize_distribution('Age')\n",
        "```\n",
        "\n",
        "- **`visualize_relationship()`**: Plots a scatter plot between two columns (e.g., Income vs. Expenditure).\n",
        "- **`visualize_distribution()`**: Plots a histogram of a specific column to understand the distribution (e.g., Age).\n",
        "\n",
        "**Expected Behavior:**\n",
        "- These functions will render visualizations directly within the Jupyter notebook or Python environment with graphical output (using matplotlib).\n",
        "\n",
        "---\n",
        "\n",
        "### Sample Workflow:\n",
        "\n",
        "```python\n",
        "# Step 1: Initialize Config and Analyzer\n",
        "config = DataConfig()\n",
        "analyzer = AdvancedDataAnalyzer(config)\n",
        "\n",
        "# Step 2: Analyze data\n",
        "stats = analyzer.calculate_statistics()\n",
        "print(\"Statistics:\", stats)\n",
        "\n",
        "# Step 3: Group data by Gender\n",
        "grouped_data = analyzer.group_by_column(\"Gender\")\n",
        "print(grouped_data)\n",
        "\n",
        "# Step 4: Filter data where Age > 50\n",
        "filtered_data = analyzer.filter_data(\"Age\", lambda x: x > 50)\n",
        "print(filtered_data)\n",
        "\n",
        "# Step 5: Save data to CSV\n",
        "analyzer.save_data(format=\"csv\", file_path=\"synthetic_data\")\n",
        "\n",
        "# Step 6: Visualize the relationship between Income and Expenditure\n",
        "analyzer.visualize_relationship(\"Income\", \"Expenditure\")\n",
        "```\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "pQ1hwsntf5uv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPLEMENTATION"
      ],
      "metadata": {
        "id": "Nr9zMNuUgI1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start here with imports"
      ],
      "metadata": {
        "id": "G51_8Jv2vdAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import pyarrow.parquet as pq\n",
        "import param\n",
        "from typing import Tuple"
      ],
      "metadata": {
        "id": "-kL176xevcXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Decorator - log_time"
      ],
      "metadata": {
        "id": "NuS2kHWqhfGK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions"
      ],
      "metadata": {
        "id": "MZrzG9vJvTFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Implementation Steps:\n",
        "- **Step 1**: Import the `time` module to measure execution time.\n",
        "- **Step 2**: Define the `log_time` function, which will act as the decorator.\n",
        "- **Step 3**: Inside `log_time`, create a `wrapper` function that accepts any arguments (`*args, **kwargs`).\n",
        "- **Step 4**: Capture the start time before the target function is executed.\n",
        "- **Step 5**: Call the target function and store the result.\n",
        "- **Step 6**: Capture the end time after the function completes.\n",
        "- **Step 7**: Calculate the execution time by subtracting the start time from the end time.\n",
        "```python\n",
        "print(f\"Execution time for {func.__name__}: {end_time - start_time:.4f} seconds\")\n",
        "```\n",
        "- **Step 8**: Print the execution time along with the function name.\n",
        "- **Step 9**: Return the result of the target function from the `wrapper`.\n",
        "\n",
        "### Using `log_time` as a Wrapper:\n",
        "- **Step 1**: Define your function normally.\n",
        "- **Step 2**: Place `@log_time` above the function definition to apply the decorator.\n",
        "- **Step 3**: When the function is called, the execution time will be printed automatically.\n",
        "Example usage\n",
        "```python\n",
        "  @log_time\n",
        "  def hello():\n",
        "    time.sleep(5)\n",
        "  hello()\n",
        "```\n",
        "  **Sample Output**\n",
        "  ```text\n",
        "  Execution time for hello: 5.0047 seconds\n",
        "```"
      ],
      "metadata": {
        "id": "0KTxTHXfhYSY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement\n",
        "***Note: For each function provide appropriate docstrings and type hinting***"
      ],
      "metadata": {
        "id": "14GPPLQloBRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decorator to log the execution time of functions\n",
        "import time\n",
        "\n",
        "def log_time(func): # !!! TODO TYPE HINTING\n",
        "    \"\"\"\n",
        "    This function creates a decorator that calculates the execution time of a\n",
        "    function and then prints it to the console.\n",
        "    \"\"\"\n",
        "    print(\"exec\")\n",
        "    def wrapper(*args, **kwargs):\n",
        "      start_time = time.time()\n",
        "      result = func()\n",
        "      end_time = time.time()\n",
        "      exec_time = end_time - start_time\n",
        "      print(f\"Execution time for {func.__name__}: {exec_time:.4f} seconds\")\n",
        "      return result\n",
        "    return wrapper\n",
        "\n",
        "@log_time\n",
        "def test():\n",
        "  time.sleep(5)\n",
        "  return \"hehe\"\n",
        "test()\n"
      ],
      "metadata": {
        "id": "D2iG88mAhmY7",
        "outputId": "3f094442-d5e0-443d-88f2-c305bbd30459",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exec\n",
            "Execution time for test: 5.0001 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hehe'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: DataConfig"
      ],
      "metadata": {
        "id": "MSINGcvGiRfN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions"
      ],
      "metadata": {
        "id": "OUs7ILOJu4Ch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. `_generate` function\n",
        "\n",
        "#### Understand following to Implement `_generate` Function:\n",
        "\n",
        "- **Step 1**: Define the `_generate` function inside the `DataConfig` class. It should be responsible for generating a synthetic dataset.\n",
        "- **Step 2**: Use the `np.random.seed()` to set the seed for random number generation to ensure reproducibility.\n",
        "- **Step 3**: Create an empty pandas DataFrame to store the generated data in `self._data` variable, use `size` arguments to pass `num_samples`.\n",
        "- **Step 4**: Use `np.random.randint()` to generate random integer values for the `Age` column based on the specified `age` range and `num_samples` use `size` arguments to pass `num_samples`.\n",
        "- **Step 5**: Use `np.random.normal()` to generate random values for `Height`, `Weight`, `Income`, and `Expenditure` based on their respective mean and standard deviation, use `size` arguments to pass `num_samples`. Only for `Expenditure` multiply it with `Income` to create final `Expenditure`\n",
        "\n",
        "\n",
        "- **Step 6**: Use `np.random.choice()`, use `size` arguments to pass `num_samples`, to randomly assign gender values from `self.gender` to the `Gender` column.\n",
        "- **Step 7**: Store the generated data in a DataFrame as the class's `_data` attribute.\n",
        "- **Step 8**: The function will automatically regenerate data when parameters like `'num_samples', 'age', 'height', 'weight', 'income', 'expenditure', 'gender'` change, thanks to the `param.depends` decorator with final arguments `watch=True`.\n",
        "\n",
        "#### Using `_generate` Function in `DataConfig`:\n",
        "- **Step 1**: The `_generate` function is automatically triggered when any of the relevant parameters (`num_samples`, `age`, etc.) change.\n",
        "- **Step 2**: The function updates the internal `_data` DataFrame with newly generated data.\n",
        "- **Step 3**: You can access the generated data by calling the `data` property of the `DataConfig` class, which will return the updated DataFrame.\n",
        "\n",
        "### 2. `_data` Property\n",
        "\n",
        "#### Understand following to create `_data` Property:\n",
        "\n",
        "- **Step 1**: `_data` is a private attribute within the `DataConfig` class that stores the generated dataset in the form of a pandas DataFrame.\n",
        "  \n",
        "- **Step 2**: The `_data` attribute is populated by the `_generate` function whenever synthetic data is created or regenerated. This ensures the data is up-to-date with the latest parameter values (e.g., `num_samples`, `age`, `height`).\n",
        "\n",
        "- **Step 3**: The `data` property method provides a public interface to access the private `_data` attribute. The reason for making `_data` private (using the underscore prefix) is to prevent direct manipulation of the DataFrame, ensuring that data can only be accessed via the `data` property.\n",
        "\n",
        "- **Step 4**: The `data` property is defined using the `@property` decorator, which makes it behave like an attribute. This allows users to access the data by simply calling `config.data` without needing to call it like a method (`config.data()`)."
      ],
      "metadata": {
        "id": "g0AaQQazi5ia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation\n",
        "***Note: For each function provide appropriate docstrings and type hinting***"
      ],
      "metadata": {
        "id": "n8YqgIbOjFM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataConfig(param.Parameterized):\n",
        "    \"\"\"\n",
        "    A class to configure and generate synthetic data.\n",
        "    This class automatically regenerates data when parameters like num_samples, age, height, etc. change.\n",
        "    Here age, height, weight, income, expenditure configs represent mean[0] and standard deviation[1] values.\n",
        "    \"\"\"\n",
        "    random_seed: int = param.Integer(default = 111, bounds = (1, None))\n",
        "    num_samples: int = param.Integer(default=1000, bounds=(1, None))\n",
        "    age: Tuple[float, float] = param.NumericTuple(default=(18.0, 80.0), length=2)\n",
        "    height: Tuple[float, float] = param.NumericTuple(default=(165.0, 10.0), length=2)\n",
        "    weight: Tuple[float, float] = param.NumericTuple(default=(70.0, 15.0), length=2)\n",
        "    income: Tuple[float, float] = param.NumericTuple(default=(50000.0, 15000.0), length=2)\n",
        "    expenditure: Tuple[float, float] = param.NumericTuple(default=(0.6, 0.1), length=2)\n",
        "    gender: str = param.ListSelector(default=['Male', 'Female', 'Other'], objects=['Male', 'Female', 'Other'])\n",
        "    _data: pd.DataFrame = param.DataFrame(default=pd.DataFrame())\n",
        "\n",
        "    def __init__(self, **params):\n",
        "        super().__init__(**params)\n",
        "        self._generate()\n",
        "\n",
        "    @param.depends(\"???\") # !!!TODO\n",
        "    def _generate(self, *events): !!! TODO PROVIDE APPROPRIATE TYPE HINTING\n",
        "        !!! TODO DOCSTRING\n",
        "\n",
        "        print(f\"New Data Generated!!!\")\n",
        "\n",
        "        !!! TODO COMPLETE THE CODE..\n",
        "\n",
        "    !!! TODO DECORATOR\n",
        "    def data(self) -> pd.DataFrame:\n",
        "        !!! TODO DOCSTRING\n",
        "        !!! TODO COMPLETE THE CODE..\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.param)"
      ],
      "metadata": {
        "id": "pmRTUti-wSBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: AdvancedDataAnalyzer"
      ],
      "metadata": {
        "id": "MgdUTUqfoqhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions"
      ],
      "metadata": {
        "id": "kFkLJrqBuaal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step-by-step guide for each function in the `AdvancedDataAnalyzer` class\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **`__init__(self, config)`**\n",
        "\n",
        "#### Purpose:\n",
        "- Initialize the `AdvancedDataAnalyzer` by taking a `DataConfig` instance as input. It accesses the data from the config instance and initializes a statistics dictionary.\n",
        "\n",
        "#### Steps:\n",
        "- **Step 1**: Store the passed `config` object in `self.config`.\n",
        "- **Step 2**: Initialize `self._data` to point to `config.data`, which contains the generated dataset.\n",
        "- **Step 3**: Initialize an empty dictionary `self._statistics` to store calculated statistics.\n",
        "\n",
        "#### Expected Usage:\n",
        "```python\n",
        "config = DataConfig()\n",
        "analyzer = AdvancedDataAnalyzer(config)\n",
        "```\n",
        "\n",
        "---\n",
        "### 2. **`data` property**\n",
        "- Returns `config.data`\n",
        "#### Steps:\n",
        "- ** Step 1**: Define `data` `property` using `@property` decorator and return `self.config.data`\n",
        "---\n",
        "\n",
        "### 3. **`calculate_statistics(self)`**\n",
        "\n",
        "#### Purpose:\n",
        "- Calculate basic statistics (mean, median, variance, correlation matrix) for numeric columns.\n",
        "\n",
        "#### Steps:\n",
        "- **Step 1**: Calculate the mean, median, variance, and correlation matrix using pandas methods (`mean()`, `median()`, `var()`, `corr()`). You might want to use `numeric_only=True` flags. `self.data.mean(numeric_only=True)`\n",
        "- **Step 3**: Use `log_time` decorator\n",
        "- **Step 2**: Store these statistics in `self._statistics` and return the dictionary.\n",
        "\n",
        "#### Expected Usage:\n",
        "```python\n",
        "stats = analyzer.calculate_statistics()\n",
        "print(stats)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **`group_by_column(self, column)`**\n",
        "\n",
        "#### Purpose:\n",
        "- Group the data by the specified column and calculate the mean for each group.\n",
        "\n",
        "#### Steps:\n",
        "- **Step 1**: Check if the column exists in `self._data`.\n",
        "- **Step 2**: Use `groupby()` on the specified column and calculate the mean for each group.\n",
        "- **Step 3**: Return the resulting grouped DataFrame.\n",
        "\n",
        "#### Expected Usage:\n",
        "```python\n",
        "grouped_data = analyzer.group_by_column('Gender')\n",
        "print(grouped_data)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **`apply_function(self, column, func)`**\n",
        "\n",
        "#### Purpose:\n",
        "- Apply a custom function to all values in the specified column and return the modified series.\n",
        "\n",
        "#### Steps:\n",
        "- **Step 1**: Check if the column exists in `self._data`.\n",
        "- **Step 2**: Use `apply()` to apply the passed function (`func`) to the column.\n",
        "- **Step 3**: Return the resulting series.\n",
        "\n",
        "#### Expected Usage:\n",
        "```python\n",
        "increased_income = analyzer.apply_function('Income', lambda x: x * 1.1)\n",
        "print(increased_income.head())\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **`lazy_filter(self, column, condition)`**\n",
        "\n",
        "#### Purpose:\n",
        "- Lazily filter rows that meet a condition in the specified column using a generator.\n",
        "\n",
        "#### Steps:\n",
        "- **Step 1**: Iterate through each row of the DataFrame using `iterrows()`.\n",
        "- **Step 2**: Apply the condition to the column.\n",
        "- **Step 3**: Yield rows that meet the condition.\n",
        "\n",
        "#### Expected Usage:\n",
        "```python\n",
        "filtered_rows = analyzer.lazy_filter('Age', lambda x: x > 50)\n",
        "for row in filtered_rows:\n",
        "    print(row)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **`filter_data(self, column, condition)`**\n",
        "\n",
        "#### Purpose:\n",
        "- Filter the data based on a condition applied to a column and return the filtered DataFrame.\n",
        "\n",
        "#### Steps:\n",
        "- **Step 1**: Check if the column exists in `self._data`.\n",
        "- **Step 2**: Use `apply()` to apply the condition and filter the rows.\n",
        "- **Step 3**: Return the filtered DataFrame.\n",
        "\n",
        "#### Expected Usage:\n",
        "```python\n",
        "filtered_data = analyzer.filter_data('Age', lambda x: x > 50)\n",
        "print(filtered_data.head())\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 8. **`visualize_relationship(self, column_x, column_y)`**\n",
        "\n",
        "#### Purpose:\n",
        "- Visualize the relationship between two columns using a scatter plot.\n",
        "\n",
        "#### Steps:\n",
        "- **Step 1**: Check if both columns exist in `self._data`.\n",
        "- **Step 2**: Use `plt.scatter()` to create the scatter plot.\n",
        "- **Step 3**: Set the x-label, y-label, and title, then call `plt.show()` to display the plot.\n",
        "\n",
        "#### Expected Usage:\n",
        "```python\n",
        "analyzer.visualize_relationship('Income', 'Expenditure')\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 9. **`visualize_distribution(self, column)`**\n",
        "\n",
        "#### Purpose:\n",
        "- Plot the distribution of a specified column using a histogram.\n",
        "\n",
        "#### Steps:\n",
        "- **Step 1**: Check if the column exists in `self._data`.\n",
        "- **Step 2**: Use `plt.hist()` to plot the histogram.\n",
        "- **Step 3**: Set the x-label, y-label, and title, then call `plt.show()` to display the plot.\n",
        "\n",
        "#### Expected Usage:\n",
        "```python\n",
        "analyzer.visualize_distribution('Age')\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 10. **`save_data(self, format, file_path, **kwargs)`**\n",
        "\n",
        "#### Purpose:\n",
        "- Save the DataFrame to disk in the specified format (`csv`, `parquet`, or `npz`).\n",
        "\n",
        "#### Steps:\n",
        "- **Step 1**: Check the format (`csv`, `parquet`, `npz`).\n",
        "- **Step 2**: Save the data using the appropriate pandas or numpy method (`to_csv()`, `to_parquet()`, `np.savez()`).\n",
        "- **Step 3**: Add the correct file extension to the `file_path`.\n",
        "- **Step 4**: Print a success message when saving is complete.\n",
        "- **Step 5**: Use `log_time` decorator\n",
        "\n",
        "#### Expected Usage:\n",
        "```python\n",
        "analyzer.save_data(format='csv', file_path=\"synthetic_data\", index = None)\n",
        "```"
      ],
      "metadata": {
        "id": "Xy50G_86rMiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation :\n",
        "\n",
        "***Note: For each function provide appropriate docstrings and type hinting***"
      ],
      "metadata": {
        "id": "pdY1rlPRrbNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedDataAnalyzer:\n",
        "    !!! DOC STRINGS FOR CLASS NEEDED\n",
        "\n",
        "    def __init__\n",
        "\n",
        "    def data\n",
        "\n",
        "    def calculate_statistics\n",
        "\n",
        "    def group_by_column\n",
        "\n",
        "    def apply_function\n",
        "\n",
        "    def lazy_filter\n",
        "\n",
        "    def filter_data\n",
        "\n",
        "    def visualize_relationship\n",
        "\n",
        "    def visualize_distribution\n",
        "\n",
        "    def save_data\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        \"\"\"String representation of the class.\"\"\"\n",
        "        return f\"AdvancedDataAnalyzer(config={str(self.config)})\"\n",
        "\n",
        "\n",
        "# Example function calls\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize DataConfig with default parameters\n",
        "    config = DataConfig()\n",
        "\n",
        "    # Create an AdvancedDataAnalyzer instance using the config\n",
        "    analyzer = AdvancedDataAnalyzer(config)\n",
        "\n",
        "    # Calculate statistics\n",
        "    stats = analyzer.calculate_statistics()\n",
        "    print(\"Statistics:\", stats)\n",
        "\n",
        "    # Group by Gender and calculate mean\n",
        "    grouped_data = analyzer.group_by_column(\"Gender\")\n",
        "    print(\"Grouped by Gender:\", grouped_data)\n",
        "\n",
        "    # Apply a function to increase all Income by 10%\n",
        "    increased_income = analyzer.apply_function(\"Income\", lambda x: x * 1.1)\n",
        "    print(\"Increased Income:\", increased_income.head())\n",
        "\n",
        "    # Save data to CSV\n",
        "    analyzer.save_data(format='csv', file_path=\"synthetic_data\")\n",
        "\n",
        "    # Visualize relationship between Income and Expenditure\n",
        "    analyzer.visualize_relationship(\"Income\", \"Expenditure\")\n",
        "\n",
        "    # Visualize Age distribution\n",
        "    analyzer.visualize_distribution(\"Age\")\n",
        "\n",
        "    # Filter data using filter_data (e.g., Age > 50)\n",
        "    filtered_data = analyzer.filter_data('Age', lambda x: x > 50)\n",
        "    print(\"Filtered Data (Age > 50):\", filtered_data.head())\n",
        "\n",
        "    # Lazily filter data using lazy_filter\n",
        "    filtered_rows = analyzer.lazy_filter('Age', lambda x: x > 50)\n",
        "    for row in filtered_rows:\n",
        "        print(row)\n",
        "        break  # Print only the first match to save output length"
      ],
      "metadata": {
        "id": "Vkt1duehbeew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST"
      ],
      "metadata": {
        "id": "wHWlg3AWgUHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run following tests\n",
        "\n",
        "**Expected Output**:\n",
        "\n",
        "```text\n",
        "test_data_generation (__main__.TestDataConfig) ... ok\n",
        "test_data_randomness (__main__.TestDataConfig) ... ok\n",
        "test_reactive_data_generation (__main__.TestDataConfig) ... ok\n",
        "test_apply_function (__main__.TestAdvancedDataAnalyzer) ... ok\n",
        "test_calculate_statistics (__main__.TestAdvancedDataAnalyzer) ... ok\n",
        "test_filter_data (__main__.TestAdvancedDataAnalyzer) ... ok\n",
        "test_group_by_column (__main__.TestAdvancedDataAnalyzer) ... ok\n",
        "test_lazy_filter (__main__.TestAdvancedDataAnalyzer) ... ok\n",
        "test_save_data (__main__.TestAdvancedDataAnalyzer) ... ok\n",
        "\n",
        "----------------------------------------------------------------------\n",
        "Ran 9 tests in 0.182s\n",
        "\n",
        "OK\n",
        "New Data Generated!!!\n",
        "New Data Generated!!!\n",
        "New Data Generated!!!\n",
        "New Data Generated!!!\n",
        "New Data Generated!!!\n",
        "New Data Generated!!!\n",
        "Execution time for calculate_statistics: 0.0035 seconds\n",
        "New Data Generated!!!\n",
        "New Data Generated!!!\n",
        "New Data Generated!!!\n",
        "New Data Generated!!!\n",
        "Data saved to test_data.csv\n",
        "Execution time for save_data: 0.0075 seconds\n",
        "```"
      ],
      "metadata": {
        "id": "dIiqq50Ptk5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pathlib\n",
        "\n",
        "class TestDataConfig(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        # Initialize DataConfig with default parameters\n",
        "        self.config = DataConfig()\n",
        "\n",
        "    def test_data_generation(self):\n",
        "        # Test if the data is generated correctly\n",
        "        data = self.config.data\n",
        "        self.assertIsInstance(data, pd.DataFrame)\n",
        "        self.assertEqual(len(data), self.config.num_samples)\n",
        "        self.assertIn('Age', data.columns)\n",
        "        self.assertIn('Height', data.columns)\n",
        "        self.assertIn('Weight', data.columns)\n",
        "        self.assertIn('Income', data.columns)\n",
        "        self.assertIn('Expenditure', data.columns)\n",
        "        self.assertIn('Gender', data.columns)\n",
        "\n",
        "    def test_reactive_data_generation(self):\n",
        "        # Change num_samples and check if data regenerates\n",
        "        old_data = self.config.data.copy()\n",
        "        self.config.num_samples = 2000\n",
        "        new_data = self.config.data\n",
        "        self.assertNotEqual(len(old_data), len(new_data))\n",
        "        self.assertEqual(len(new_data), 2000)\n",
        "\n",
        "    def test_data_randomness(self):\n",
        "        # Check if data generation respects random_seed\n",
        "        self.config.random_seed = 123\n",
        "        data1 = self.config.data.copy()\n",
        "        self.config.random_seed = 123\n",
        "        data2 = self.config.data.copy()\n",
        "        pd.testing.assert_frame_equal(data1, data2)\n",
        "\n",
        "\n",
        "class TestAdvancedDataAnalyzer(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        # Initialize DataConfig and AdvancedDataAnalyzer with default parameters\n",
        "        self.config = DataConfig()\n",
        "        self.analyzer = AdvancedDataAnalyzer(self.config)\n",
        "\n",
        "    def test_calculate_statistics(self):\n",
        "        # Test if statistics are calculated correctly\n",
        "        stats = self.analyzer.calculate_statistics()\n",
        "        self.assertIsInstance(stats, dict)\n",
        "        self.assertIn('mean', stats)\n",
        "        self.assertIn('median', stats)\n",
        "        self.assertIn('variance', stats)\n",
        "        self.assertIn('correlation_matrix', stats)\n",
        "\n",
        "    def test_group_by_column(self):\n",
        "        # Test if data is grouped correctly by Gender\n",
        "        grouped_data = self.analyzer.group_by_column('Gender')\n",
        "        self.assertIsInstance(grouped_data, pd.DataFrame)\n",
        "        self.assertIn('Age', grouped_data.columns)\n",
        "\n",
        "    def test_apply_function(self):\n",
        "        # Test applying a custom function to the Income column\n",
        "        increased_income = self.analyzer.apply_function('Income', lambda x: x * 1.1)\n",
        "        self.assertIsInstance(increased_income, pd.Series)\n",
        "        self.assertTrue(np.allclose(increased_income.values, self.config.data['Income'] * 1.1))\n",
        "\n",
        "    def test_filter_data(self):\n",
        "        # Test filtering data where Age > 50\n",
        "        filtered_data = self.analyzer.filter_data('Age', lambda x: x > 50)\n",
        "        self.assertTrue((filtered_data['Age'] > 50).all())\n",
        "\n",
        "    def test_lazy_filter(self):\n",
        "        # Test lazy filtering where Age > 50\n",
        "        filtered_rows = list(self.analyzer.lazy_filter('Age', lambda x: x > 50))\n",
        "        self.assertTrue(all(row['Age'] > 50 for row in filtered_rows))\n",
        "\n",
        "    def test_save_data(self):\n",
        "        #  Test saving data in CSV format by checking file existence using pathlib\n",
        "        file_path = pathlib.Path(\"test_data.csv\")\n",
        "        # Ensure the file does not exist before the test\n",
        "        if file_path.exists():\n",
        "            file_path.unlink()\n",
        "        # Save data in CSV format\n",
        "        self.analyzer.save_data(format='csv', file_path=\"test_data\")\n",
        "\n",
        "        # Check if the file exists\n",
        "        self.assertTrue(file_path.exists())\n",
        "\n",
        "        # Cleanup the file after the test\n",
        "        file_path.unlink()\n",
        "\n",
        "\n",
        "# Run the tests in Jupyter Notebook\n",
        "def run_tests():\n",
        "    suite = unittest.TestLoader().loadTestsFromTestCase(TestDataConfig)\n",
        "    suite.addTests(unittest.TestLoader().loadTestsFromTestCase(TestAdvancedDataAnalyzer))\n",
        "    unittest.TextTestRunner(verbosity=2).run(suite)\n",
        "\n",
        "# Execute tests\n",
        "run_tests()"
      ],
      "metadata": {
        "id": "G-ehJO3KbfLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tPe1xr0OcZW3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}